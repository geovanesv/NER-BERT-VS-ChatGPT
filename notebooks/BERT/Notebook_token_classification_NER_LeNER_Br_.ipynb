{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLoRwNce-vzX"
      },
      "source": [
        "# Ajuste da vers\u00e3o especializada do modelo de linguagem BERTimbau em uma tarefa de classifica\u00e7\u00e3o de tokens (NER) com o dataset LeNER-Br"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WT-HWVQ7VKe"
      },
      "source": [
        "- **Credit**: this notebook is copied/pasted with small changes from [PyTorch Examples](https://huggingface.co/docs/transformers/notebooks#pytorch-examples) of Hugging Face (notebook [token_classification.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb)).\n",
        "- **Author**: [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/)\n",
        "- **Date**: 12/20/2021\n",
        "- **Blog post**: [NLP | Modelos e Web App para Reconhecimento de Entidade Nomeada (NER) no dom\u00ednio jur\u00eddico brasileiro](https://medium.com/@pierre_guillou/nlp-modelos-e-web-app-para-reconhecimento-de-entidade-nomeada-ner-no-dom%C3%ADnio-jur%C3%ADdico-b658db55edfb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "## Vis\u00e3o geral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTvCIuqgEj0A"
      },
      "source": [
        "Neste notebook, veremos como ajustar um dos modelos [\ud83e\udd17 Transformers](https://github.com/huggingface/transformers) para uma tarefa de classifica\u00e7\u00e3o de token, que \u00e9 a tarefa de prever um r\u00f3tulo para cada token .\n",
        "\n",
        "![Widget inference representing the NER task](https://github.com/huggingface/notebooks/raw/8044bbce25bed20a79e5488040a41d3c32575cec/examples/images/token_classification.png)\n",
        "\n",
        "As tarefas de classifica\u00e7\u00e3o de token mais comuns s\u00e3o:\n",
        "\n",
        "- NER (reconhecimento de entidade nomeada) Classifica as entidades no texto (pessoa, organiza\u00e7\u00e3o, localiza\u00e7\u00e3o...).\n",
        "- POS (marca\u00e7\u00e3o gramatical) Classifique gramaticalmente os tokens (substantivo, verbo, adjetivo...)\n",
        "- Chunk (Chunking) Classificar gramaticalmente os tokens e agrup\u00e1-los em \u201cpeda\u00e7os\u201d que v\u00e3o juntos\n",
        "\n",
        "Veremos como carregar facilmente um conjunto de dados para esses tipos de tarefas e usar a API `Trainer` para ajustar um modelo nele."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "Este notebook foi desenvolvido para ser executado em qualquer tarefa de classifica\u00e7\u00e3o de token, com qualquer ponto de verifica\u00e7\u00e3o de modelo do [Model Hub](https://huggingface.co/models), desde que esse modelo tenha uma vers\u00e3o com um cabe\u00e7ote de classifica\u00e7\u00e3o de token e um tokenizer r\u00e1pido (verifique [esta tabela](https://huggingface.co/transformers/index.html#bigtable) se for esse o caso). Poder\u00e3o ser necess\u00e1rios apenas alguns pequenos ajustes se voc\u00ea decidir usar um conjunto de dados diferente daquele usado aqui. Dependendo do modelo e da GPU que voc\u00ea est\u00e1 usando, pode ser necess\u00e1rio ajustar o tamanho do lote para evitar erros de falta de mem\u00f3ria. Defina esses tr\u00eas par\u00e2metros e o resto do notebook dever\u00e1 funcionar sem problemas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbVxQFOV8B21"
      },
      "source": [
        "## Configura\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "\n",
        "# model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
        "# model_checkpoint = \"neuralmind/bert-large-portuguese-cased\"\n",
        "model_checkpoint = \"pierreguillou/bert-base-cased-pt-lenerbr\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFz2diizOhKz",
        "outputId": "e46b594c-f8cc-4cba-9b15-fd460e929771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EZW0-ff70vJ"
      },
      "source": [
        "Se voc\u00ea estiver abrindo este Notebook no colab, provavelmente precisar\u00e1 instalar \ud83e\udd17 Transformers e \ud83e\udd17 Datasets. Remova o coment\u00e1rio da c\u00e9lula a seguir e execute-a."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOsHUjgdIrIW",
        "outputId": "ce68a155-d4fc-4074-ec9f-72aca52e4924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=d52c675caa358b4cb4cf2a31aec28e7d505ab5600b1e10b3650669302f551114\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, seqeval, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 huggingface-hub-0.17.2 multiprocess-0.70.15 seqeval-1.2.2 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AncKkajgf_ww",
        "outputId": "70f411f8-3b37-45e5-ab8a-b451ff2ba9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/258.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m30.7/258.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m122.9/258.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.23.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, transformers\n",
            "Successfully installed safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U accelerate\n",
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19JACx0TEjz5"
      },
      "source": [
        "Se voc\u00ea estiver abrindo este notebook localmente, certifique-se de que seu ambiente tenha uma instala\u00e7\u00e3o da \u00faltima vers\u00e3o dessas bibliotecas.\n",
        "\n",
        "Para poder compartilhar seu modelo com a comunidade e gerar resultados como o mostrado na imagem abaixo por meio da API de infer\u00eancia, h\u00e1 mais alguns passos a seguir.\n",
        "\n",
        "Primeiro voc\u00ea deve armazenar seu token de autentica\u00e7\u00e3o do site Hugging Face (inscreva-se [aqui](https://huggingface.co/join) se ainda n\u00e3o o fez!), em seguida, execute a seguinte c\u00e9lula e insira seu nome de usu\u00e1rio e senha:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCR-Kws7Ejz7"
      },
      "source": [
        "Ent\u00e3o voc\u00ea precisa instalar o Git-LFS. Remova o coment\u00e1rio das seguintes instru\u00e7\u00f5es:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gipmah9QeWjY",
        "outputId": "74731dab-cca4-4541-9923-579a71aa067e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK7bIVJ-Ejz8"
      },
      "source": [
        "Certifique-se de que sua vers\u00e3o do Transformers seja pelo menos 4.11.0, j\u00e1 que a funcionalidade foi introduzida nessa vers\u00e3o:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrDA_lWtEjz8",
        "outputId": "07b1f092-f742-401d-fece-efc5390eb8c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.33.2\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqlk0kFyRRZY",
        "outputId": "3ccc6fff-5a47-4a6e-a3c8-514eae3adda8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.5\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "\n",
        "print(datasets.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6L3zDsM8OnI"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3fNgz678QiE"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFASsisvIrIb"
      },
      "source": [
        "Voc\u00ea pode encontrar uma vers\u00e3o de script deste notebook para ajustar seu modelo de forma distribu\u00edda usando v\u00e1rias GPUs ou TPUs [aqui](https://github.com/huggingface/transformers/tree/master/examples/token-classification) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Carregando o conjunto de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "Usaremos a biblioteca [\ud83e\udd17 Datasets](https://github.com/huggingface/datasets) para baixar os dados e obter a m\u00e9trica que precisamos usar para avalia\u00e7\u00e3o (para comparar nosso modelo com o benchmark). Isso pode ser feito facilmente com as fun\u00e7\u00f5es `load_dataset` e `load_metric`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "Para nosso exemplo aqui, usaremos o [conjunto de dados LeNER-Br](https://huggingface.co/datasets/lener_br). O notebook deve funcionar com qualquer conjunto de dados de classifica\u00e7\u00e3o de token fornecido pela biblioteca \ud83e\udd17 Datasets. Se voc\u00ea estiver usando seu pr\u00f3prio conjunto de dados definido a partir de um arquivo JSON ou CSV (consulte a [documenta\u00e7\u00e3o de conjuntos de dados](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) para saber como carreg\u00e1-los ), poder\u00e1 ser necess\u00e1rio alguns ajustes nos nomes das colunas utilizadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "5f5c6c59b1da49fbab075321925c4ef4",
            "c3eae4a5baaf4544b7d73430180211ab",
            "6841f3d5169a4152b3a552a2e9727de9",
            "675c9210625f4ec3b25e0f66ca631365",
            "dc4b2b45954d44848b32b22300faf119",
            "dbeed6cde9ce4514a8f2f3e7f9174249",
            "450b654f77294aa8bafb7b80f2f14b4e",
            "7c815e2b365542c89129570c785cf946",
            "59b28d3a9e904a74a0e24d06c3451eda",
            "404b41cb3d864bde8d83fd9164006ed8",
            "1990022e51e6411e8a7cce036204b298",
            "f058ba801fc4423f910d708c324a3589",
            "82781bc471524cdb833ad59f314392ef",
            "3b704306ea2c443780a11d7a53eeec60",
            "db36313bb35d49e0b21a778e363b6d15",
            "36a7affe84784dcc86bd67440f85aa49",
            "3d1a307aeaaa4a5aada53991d617bfe9",
            "5e9ccf5e392a47768b4e387cba741c03",
            "1e57dc0fc7e14957bbbc28b30cc73a41",
            "eb3311d38f0f40b89f6d3fc1dc4beed4",
            "edb1f4130cc14145b56f8dbea45273b5",
            "ef7532bad53c47bfbe6ccad11a5cb9d9",
            "660d6ee9f52341a899c034eeff8a7602",
            "dd161d01b542458f88d940f185093c15",
            "3a3b1dd8c44a4956833dd5dbe253c82e",
            "11a40bef79174639916aebb0a03cf97b",
            "c8f182183fd54d99aa444896a9dea212",
            "a709f201b0084d97ab9256cae3fcebee",
            "5c3a362234bf4a12816617366752ae40",
            "905bcf5364bd418c894cf86a3e3b9668",
            "a9ad4ca71dce41829618130f83ed65a5",
            "7ba93bbedf7545b3809a4449ebeaace7",
            "c9f47ccf3d014790b513ee5a8eb639e7",
            "9b27b584a0eb4cf98dd8e554bb20ca00",
            "e6b68a2c8de1420da87e77e7d1a627ce",
            "99ae1d8bee8f4c3aa47fba0315ed5901",
            "1a4e678685be4556aaa97f6ccb30c007",
            "25f3ce29a92c412b8346e3fde6d0574d",
            "bcac101a8fe044b49dd021fece6b6583",
            "3eba478361644b7c8627d5336f62a192",
            "5be29c95d03a49a1969e37cf76b636ce",
            "586b37fcc58245dea953dc77aacd4beb",
            "323b10e021a04e0ca900c20b94c620a5",
            "2c04bcdb73ae4d1e8b6b4afaa61dd12a",
            "09e83ca414bf41e4ba443326b208fec2",
            "1b97fb662fcb4f0981eb4adc1254eaed",
            "67c02d87b788403e8837706d419492b2",
            "418e4c92e1224eafa5792685fe798185",
            "a544654e77fd4c2fbf95473e92c62da4",
            "635d49592fe14a4f9a3cbd42fafa5891",
            "65a3c26a19684d11bef6d1406fe990e4",
            "681dd5d5f4f14578a189b8fc85b82194",
            "b81cf96540e24f3ea5370432c04aff8b",
            "7a980dd6e1a142268fa65d62bd93b848",
            "3b60b461de4643e89a84184161816c18",
            "cde70797725a40db9420cd1220c26870",
            "0dc6999027c74069b239584cb3f079fb",
            "7458926087df4e1d94b1cb0f5e1e63ac",
            "9701c6df2af14699979e8409c257078a",
            "72e1039d0bd2414baf48eae53f06e824",
            "b18685492ec84201ad9a4fa10bbd18b1",
            "1b165efb452b4a79a536ddd98224dcd5",
            "32727e2d139a454796352bdbcebb4ad4",
            "ad335827c2cf4d589ed7e6cbf3b2f0bd",
            "5313fc180bf24e468bc4839f50f6beeb",
            "f0c45654effe4c9690b490b5720d8e72",
            "e7a9454601654bf79fa525e82bd668af",
            "aa4cf784a6fa465d8ca986177204ae89",
            "255357df4dc64c98b1d65a293aa139d7",
            "e79730a9141d4e978573aa5b63716070",
            "89fceb43d3e045b2a1cd374038b90803",
            "f5e2d7fd62b34fbdbce938d70bea2c52",
            "de6a33ab178546e09c34702fe11b5ef4",
            "69148ee3c18643468c7267f7891617e7",
            "9978a18520364605a891f9fa4b6e5ca2",
            "5e0517dd1939418885b368abd6fa5eb3",
            "029e8a9e118a4b98aabe229e5f5ce409",
            "932607051e934a39b19e6e5c9da2fa5b",
            "089eacef0db1498f8969aebb80db4e15",
            "13845355ba474a9392ed5d318ad063d3",
            "847c135ed3bc433392e3f513c687d7aa",
            "068adbf6ee9b42f68339b4b53ea9c104",
            "08e3300bd096417293e06aaf995828fa",
            "cf4856c7a7cb478088023d2c983860ce",
            "fba55c78d3b04b94bf11dc161c825f72",
            "3cf08ce954e44513ab2a2faf6f239f9b",
            "fa0c650f4064497faa4b016326669b32",
            "11a33b040b2945b2803e0581ef3c5990",
            "4b2008b176da41f5a31458ae17b1db9e",
            "da9695707b354e6ba3e54053c1292833",
            "3ee8a4dee93344a78434765cf2f4073d",
            "5b2bde402a9743c9b0449a31f4b9b890",
            "833d1b33db0e47c6bd9f610f126fe537",
            "be2f6d27f9e84de984c22423bcb6e2f7",
            "456ef91315d54b1b9a826fd257145dcc",
            "56d413ef76e64cdfb116a6767c0f78b0",
            "2f0f5138fd2948948dee69222a6c9f48",
            "62c86cf1b42c4925ae430eedefadd2f4",
            "fff42b63022a46f790b608b3f1627736",
            "67a4b715f709433cb50047ca16d1b034",
            "77d4bcf7008045f6b2f1a324150f57df",
            "6f6fdc940df04029a12a329ce2e19a26",
            "af042bfb6aff46bd8b93d4ee9179ae82",
            "4df3a307ad634914bf63ad02384d574c",
            "bad812b5af2245ab8293a46d79f4b3f4",
            "413eb167ee334775b09cba9dc27e98dc",
            "1f98e7dbc5f44d25b8005f7befafce2b",
            "3e9dbe8c7b744b2592addee048461fb5",
            "c319ac896ab84afd8c6498db21ddd916",
            "851ebc534d6a417ba66b31816f2eafa1",
            "90014bbf4292465bbe22f1a73feaae08",
            "65c8605131204de880b7cd4774b9cb1a",
            "f6d84ce6f26a4b3f9677255d0cf3285d",
            "56ff82ccb9c34a61a9d21b36500a3120",
            "89413d68887741b79b32677ad0c1fa73",
            "7208bda2d9c44d819330b78bb5b1bbbc",
            "b23dd2c0e5644fa59c80a3b35583cc32",
            "81684f22e57e40809dd458e4cc1bbd2f",
            "089a076b7d704325b5c0b4082ff237ad",
            "feaa78999017482dbfc9eaa88b23e466",
            "1db535b4890345859ea61a2aed4869da"
          ],
          "height": 369
        },
        "id": "s_AY1ATSIrIq",
        "outputId": "38fd8512-98c9-4c15-c75a-097b76a94c57"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.84k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f5c6c59b1da49fbab075321925c4ef4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/3.25k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f058ba801fc4423f910d708c324a3589"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/6.10k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "660d6ee9f52341a899c034eeff8a7602"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b27b584a0eb4cf98dd8e554bb20ca00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09e83ca414bf41e4ba443326b208fec2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/70.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cde70797725a40db9420cd1220c26870"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/94.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7a9454601654bf79fa525e82bd668af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "932607051e934a39b19e6e5c9da2fa5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/7828 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b2008b176da41f5a31458ae17b1db9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/1177 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67a4b715f709433cb50047ca16d1b034"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1390 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90014bbf4292465bbe22f1a73feaae08"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "datasets = load_dataset(\"lener_br\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "O objeto `datasets` em si \u00e9 [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), que cont\u00e9m uma chave para o conjunto de treinamento, valida\u00e7\u00e3o e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCW9RM6XEj0E",
        "outputId": "2331f9a1-c2c0-4450-9614-5dfe9ab19d90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 7828\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 1177\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 1390\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EFq3z-ZEj0H"
      },
      "source": [
        "Podemos ver que todos os conjuntos de treinamento, valida\u00e7\u00e3o e teste t\u00eam uma coluna para os tokens (os textos de entrada divididos em palavras) e uma coluna de r\u00f3tulos para cada tipo de tarefa que introduzimos anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtYfeHIrIz"
      },
      "source": [
        "Para acessar um elemento real, voc\u00ea precisa primeiro selecionar uma divis\u00e3o e depois fornecer um \u00edndice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6HrpprwIrIz",
        "outputId": "0a37e181-5969-479b-c941-13c4b3790572"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EMENTA',\n",
              "  ':',\n",
              "  'APELA\u00c7\u00c3O',\n",
              "  'C\u00cdVEL',\n",
              "  '-',\n",
              "  'A\u00c7\u00c3O',\n",
              "  'DE',\n",
              "  'INDENIZA\u00c7\u00c3O',\n",
              "  'POR',\n",
              "  'DANOS',\n",
              "  'MORAIS',\n",
              "  '-',\n",
              "  'PRELIMINAR',\n",
              "  '-',\n",
              "  'ARGUIDA',\n",
              "  'PELO',\n",
              "  'MINIST\u00c9RIO',\n",
              "  'P\u00daBLICO',\n",
              "  'EM',\n",
              "  'GRAU',\n",
              "  'RECURSAL',\n",
              "  '-',\n",
              "  'NULIDADE',\n",
              "  '-',\n",
              "  'AUS\u00caNCIA',\n",
              "  'DE',\n",
              "  'INTERVEN\u00c7\u00c3O',\n",
              "  'DO',\n",
              "  'PARQUET',\n",
              "  'NA',\n",
              "  'INST\u00c2NCIA',\n",
              "  'A',\n",
              "  'QUO',\n",
              "  '-',\n",
              "  'PRESEN\u00c7A',\n",
              "  'DE',\n",
              "  'INCAPAZ',\n",
              "  '-',\n",
              "  'PREJU\u00cdZO',\n",
              "  'EXISTENTE',\n",
              "  '-',\n",
              "  'PRELIMINAR',\n",
              "  'ACOLHIDA',\n",
              "  '-',\n",
              "  'NULIDADE',\n",
              "  'RECONHECIDA',\n",
              "  '.'],\n",
              " 'ner_tags': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  2,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahSl4rFpEj0H"
      },
      "source": [
        "The labels are already coded as integer ids to be easily usable by our model, but the correspondence with the actual categories is stored in the `features` of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oedB8Q--Ej0H",
        "outputId": "1f27eddc-4085-4cb9-f152-4659670624d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['O', 'B-ORGANIZACAO', 'I-ORGANIZACAO', 'B-PESSOA', 'I-PESSOA', 'B-TEMPO', 'I-TEMPO', 'B-LOCAL', 'I-LOCAL', 'B-LEGISLACAO', 'I-LEGISLACAO', 'B-JURISPRUDENCIA', 'I-JURISPRUDENCIA'], id=None), length=-1, id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "datasets[\"train\"].features[f\"ner_tags\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1udRoR7CEj0I"
      },
      "source": [
        "Portanto, para as tags NER, 0 corresponde a 'O', 1 a 'B-PER' etc... Al\u00e9m de 'O' (o que significa nenhuma entidade especial), existem quatro r\u00f3tulos para NER aqui, cada um prefixado com 'B-' (para iniciante) ou 'I-' (para intermedi\u00e1rio), que indica se o token \u00e9 o primeiro do grupo atual com o r\u00f3tulo ou n\u00e3o:\n",
        "- 'PESSOA' for person\n",
        "- 'ORGANIZACAO' for organization\n",
        "- 'LOCAL' for location\n",
        "- ...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "typxzo_DEj0I"
      },
      "source": [
        "Como os r\u00f3tulos s\u00e3o listas de `ClassLabel`, os nomes reais dos r\u00f3tulos est\u00e3o aninhados no atributo `feature` do objeto acima:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ali52SGLEj0I",
        "outputId": "4f5a879e-2623-4062-e166-b4e7bcc47881"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'B-ORGANIZACAO',\n",
              " 'I-ORGANIZACAO',\n",
              " 'B-PESSOA',\n",
              " 'I-PESSOA',\n",
              " 'B-TEMPO',\n",
              " 'I-TEMPO',\n",
              " 'B-LOCAL',\n",
              " 'I-LOCAL',\n",
              " 'B-LEGISLACAO',\n",
              " 'I-LEGISLACAO',\n",
              " 'B-JURISPRUDENCIA',\n",
              " 'I-JURISPRUDENCIA']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
        "label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "Para ter uma ideia da apar\u00eancia dos dados, a fun\u00e7\u00e3o a seguir mostrar\u00e1 alguns exemplos escolhidos aleatoriamente no conjunto de dados (decodificando automaticamente os r\u00f3tulos de passagem)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        },
        "id": "SZy5tRB_IrI7",
        "outputId": "57927b07-6b3c-4e75-b23e-a1cd385a2bff",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tokens</th>\n",
              "      <th>ner_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3462</td>\n",
              "      <td>[7, -, N\u00e3o, ofende, o, art, ., 59, do, CP, a, fixa\u00e7\u00e3o, da, pena-base, acima, do, m\u00ednimo, legal, se, as, circunst\u00e2ncias, judiciais, desfavor\u00e1veis, resultaram, da, an\u00e1lise, das, condi\u00e7\u00f5es, pessoais, do, recorrente, ,, como, sua, conduta, social, e, personalidade, ,, bem, como, das, circunst\u00e2ncias, e, consequ\u00eancias, do, delito, ,, que, evidenciaram, sua, alta, culpabilidade, e, a, maior, necessidade, de, reprova\u00e7\u00e3o, e, preven\u00e7\u00e3o, do, crime, ,, n\u00e3o, prosperando, a, alega\u00e7\u00e3o, de, utiliza\u00e7\u00e3o, ,, na, senten\u00e7a, condenat\u00f3ria, ,, de, elementos, constitutivos, do, pr\u00f3prio, tipo, penal, .]</td>\n",
              "      <td>[O, O, O, O, O, B-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7264</td>\n",
              "      <td>[13, .]</td>\n",
              "      <td>[O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4519</td>\n",
              "      <td>[Decis\u00e3o, :, O, Tribunal, ,, por, unanimidade, ,, rejeitou, os, embargos, de, declara\u00e7\u00e3o, ,, nos, termos, do, voto, do, relator, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2271</td>\n",
              "      <td>[Na, aferi\u00e7\u00e3o, da, legitimidade, passiva, deve-se, tomar, por, base, o, direito, abstratamente, invocado, e, a, pertin\u00eancia, subjetiva, entre, o, pedido, e, as, partes, chamadas, em, ju\u00edzo, ,, analisada, conforme, a, Teoria, da, Asser\u00e7\u00e3o, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5500</td>\n",
              "      <td>[Habeas, corpus, n\u00e3o, conhecido, .]</td>\n",
              "      <td>[O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6213</td>\n",
              "      <td>[Conforme, esclarecido, pelo, Relator, daquele, processo, administrativo, ,, o, i, ., Min, ., Aldir, Passarinho, Junior, ,, \u201c, as, decis\u00f5es, dos, TRF, 's, que, impliquem, aumento, de, despesa, ,, para, que, tenham, efic\u00e1cia, ,, devem, ser, submetidas, \u00e0, homologa\u00e7\u00e3o, do, Colegiado, do, CJF, ,, a, teor, da, exig\u00eancia, do, art, ., 5\u00ba, ,, IV, ,, da, Lei, n\u00ba, 8.472, ,, de, 14, de, outubro, de, 1992, ,, e, do, art, ., 4\u00ba, ,, IV, ,, do, Regimento, Interno, do, CJF, \u201d, (, fl, ., 220, ), .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-PESSOA, I-PESSOA, I-PESSOA, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORGANIZACAO, O, O, O, O, O, O, B-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, O, O, B-TEMPO, I-TEMPO, I-TEMPO, I-TEMPO, I-TEMPO, O, O, O, B-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3026</td>\n",
              "      <td>[O, Estado, quer, ,, al\u00e9m, da, punibilidade, ,, a, '', utilidade, '', da, persecu\u00e7\u00e3o, penal, acoplada, \u00e0, efetividade, administrativa, ,, ou, seja, ,, poder, exigir, do, agente, ,, apto, em, inspe\u00e7\u00e3o, de, sa\u00fade, ,, o, cumprimento, do, restante, do, Servi\u00e7o, Militar, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5333</td>\n",
              "      <td>[Por\u00e9m, ,, se, a, inten\u00e7\u00e3o, tiver, o, escopo, de, vedar, qualquer, coment\u00e1rio, do, juiz, a, respeito, das, teses, levantadas, pela, defesa, ,, ignorando-as, por, completo, ,, atinge-se, a, inconstitucionalidade, ,, pois, fere, a, plenitude, de, defesa, e, o, preceito, constitucional, de, que, toda, decis\u00e3o, do, Poder, Judici\u00e1rio, deve, ser, fundamentada, ,, n\u00e3o, podendo, haver, cerceamento, por, mando, da, lei, ordin\u00e1ria, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2751</td>\n",
              "      <td>[Relator, :, William, de, Oliveira, Barros, ,, Julgamento, :, 10/09/2014, ,, Publica\u00e7\u00e3o, Dje, :, 23/09/2014, ), .]</td>\n",
              "      <td>[O, O, B-PESSOA, I-PESSOA, I-PESSOA, I-PESSOA, O, O, O, B-TEMPO, O, O, O, O, B-TEMPO, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6963</td>\n",
              "      <td>[Desnecess\u00e1ria, a, remessa, dos, autos, ao, Minist\u00e9rio, P\u00fablico, do, Trabalho, ,, consoante, o, art, ., 83, ,, \u00a7, 2.\u00ba, ,, II, ,, do, RITST, .]</td>\n",
              "      <td>[O, O, O, O, O, O, B-ORGANIZACAO, I-ORGANIZACAO, I-ORGANIZACAO, I-ORGANIZACAO, O, O, O, B-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, I-LEGISLACAO, O]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_random_elements(datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "##Pr\u00e9-processando os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Antes de podermos alimentar esses textos em nosso modelo, precisamos pr\u00e9-process\u00e1-los. Isso \u00e9 feito por um \ud83e\udd17 Transformers `Tokenizer` que ir\u00e1 (como o nome indica) tokenizar as entradas (incluindo a convers\u00e3o dos tokens em seus IDs correspondentes no vocabul\u00e1rio pr\u00e9-treinado) e coloc\u00e1-los em um formato que o modelo espera, bem como gerar o outras entradas que o modelo requer.\n",
        "\n",
        "Para fazer tudo isso, instanciamos nosso tokenizer com o m\u00e9todo `AutoTokenizer.from_pretrained`, que ir\u00e1 garantir:\n",
        "\n",
        "- obtemos um tokenizer que corresponde \u00e0 arquitetura do modelo que queremos usar,\n",
        "- baixamos o vocabul\u00e1rio usado no pr\u00e9-treinamento deste ponto de verifica\u00e7\u00e3o espec\u00edfico.\n",
        "\n",
        "Esse vocabul\u00e1rio ser\u00e1 armazenado em cache, portanto n\u00e3o ser\u00e1 baixado novamente na pr\u00f3xima vez que executarmos a c\u00e9lula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "ac36423e20fb483497dc07000f4c7624",
            "6c228d05c0b349fd9b981fa6b5adea3f",
            "b7e2f1465d884eb18f08596d7a248ac6",
            "ac13066955b14f94b4fa3c23877af978",
            "1d3e4748a0d749daa85b66e8e801b82d",
            "f81e3e582b17475cb046d3260f2c6c61",
            "a92f5d45e2634639af271216d918fb79",
            "e44031b945ec412a830b284c96a64afe",
            "e4b9fdb2b7a54ffe8eb00fa73fdfdc48",
            "9148bb3c381543f0af6e37a99dd374f5",
            "5247a1091f2a45b48ed95b53d2a63535",
            "96972f6b9f0648fd9cf4c6ec448c72a7",
            "c0984a057a0a404fa5fb4fa7afd05de2",
            "c0311631e4934200bcf8de17aa12f711",
            "aa0950c62fa54623a0d346479161b19a",
            "356857f362984c1489990db877e90ea0",
            "a3f853991c704fc5815ac04a6d5d05f1",
            "64c7f77c68044a47a85fed1df2191830",
            "857bb79122a34a589fd1e29379433768",
            "ad7e097fab954487833b445ef7c57398",
            "1dd2816edb6144c5a2bde3b5cf5553bc",
            "e27b3b446a854d1fa31cec90a9f991f6",
            "7331f7a7bc4f48569872a2197431954a",
            "8a41fa7f476548e2ac2cc03ac4b5ad01",
            "0f94daa9fa3d43c99620a6ef56e248d9",
            "c8da03a7805449ada4bb1c022777759f",
            "88d388b67f1b407881bb42d77982c71d",
            "672cdc370cb3475f96bbd894027bed9c",
            "e547f8a78ba04782a24619cc2302e065",
            "9c18efafb4ab471994b1160b96b79010",
            "0970cfb7a0ab421bb3a06be34e6851b7",
            "7c2790894c69428b9fb9a82c50f912bb",
            "a1f72bd2c9c7478ca3aa13e70ebf949c",
            "4f13261a82194c0d82d4a010c513ee72",
            "6f3739e04d8f44ff89776ef4d379fdf9",
            "b136e81833534bcea9508429558669d8",
            "1c28f6e03f8744528af9b740eaa72277",
            "00941225d7be44dc894d3980b80a7106",
            "ede9b02e7b864b958f37858fbb8d6efc",
            "f2cd959fad7f47feac853a58f529c4b4",
            "d2aff72a5e7941c9901c637ca96b1b88",
            "ae2f71b108374e62b2f86e10db30b1bb",
            "d7be116b7926451f82f610799d554b0a",
            "175b63e738dd4e0cb109abbac9ec9188"
          ],
          "height": 145
        },
        "id": "eXNLu_-nIrJI",
        "outputId": "b5211d06-0951-4d6f-f375-20280efcd9fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (\u2026)okenizer_config.json:   0%|          | 0.00/530 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac36423e20fb483497dc07000f4c7624"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (\u2026)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96972f6b9f0648fd9cf4c6ec448c72a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (\u2026)/main/tokenizer.json:   0%|          | 0.00/438k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7331f7a7bc4f48569872a2197431954a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (\u2026)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f13261a82194c0d82d4a010c513ee72"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "A afirma\u00e7\u00e3o a seguir garante que nosso tokenizer \u00e9 um tokenizer r\u00e1pido (apoiado por Rust) da biblioteca \ud83e\udd17 Tokenizers. Esses tokenizadores r\u00e1pidos est\u00e3o dispon\u00edveis para quase todos os modelos e precisaremos de alguns dos recursos especiais que eles possuem para nosso pr\u00e9-processamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-C94WcmEj0K"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSqpo1sSEj0K"
      },
      "source": [
        "Voc\u00ea pode verificar quais tipos de modelos t\u00eam um tokenizer r\u00e1pido dispon\u00edvel e quais n\u00e3o t\u00eam na [grande tabela de modelos](https://huggingface.co/transformers/index.html#bigtable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "Voc\u00ea pode chamar esse tokenizer diretamente em uma frase:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5hBlsrHIrJL",
        "outputId": "c5817701-b510-440f-fb1d-89eab1465dd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 15044, 22280, 117, 12230, 145, 847, 3185, 22279, 5440, 1710, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "tokenizer(\"Hello, this is one sentence!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sWvL3yQEj0L"
      },
      "source": [
        "Dependendo do modelo selecionado, voc\u00ea ver\u00e1 chaves diferentes no dicion\u00e1rio retornado pela c\u00e9lula acima. Eles n\u00e3o importam muito para o que estamos fazendo aqui (apenas saiba que s\u00e3o exigidos pelo modelo que instanciaremos mais tarde). Voc\u00ea pode aprender mais sobre eles [neste tutorial](https://huggingface.co/transformers/ preprocessing.html) se voc\u00ea estiver interessado.\n",
        "\n",
        "Se, como \u00e9 o caso aqui, suas entradas j\u00e1 foram divididas em palavras, voc\u00ea deve passar a lista de palavras para o seu tokenzier com o argumento `is_split_into_words=True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQag7Q9jEj0L",
        "outputId": "f4e36a9e-56d5-4116-fbb3-ec3e676a4701"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 15044, 22280, 117, 12230, 145, 847, 3185, 22279, 5440, 1710, 139, 863, 284, 19124, 2702, 824, 22281, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenizer([\"Hello\", \",\", \"this\", \"is\", \"one\", \"sentence\", \"split\", \"into\", \"words\", \".\"], is_split_into_words=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0D_xR-NEj0L"
      },
      "source": [
        "Observe que os transformadores geralmente s\u00e3o pr\u00e9-treinados com tokenizadores de subpalavras, o que significa que mesmo que suas entradas j\u00e1 tenham sido divididas em palavras, cada uma dessas palavras poder\u00e1 ser dividida novamente pelo tokenizador. Vejamos um exemplo disso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWTsor4rEj0M",
        "outputId": "09175427-d739-4fd5-869c-d55b5ba57bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['V.v', 'APELA\u00c7\u00c3O', 'C\u00cdVEL', '-', 'NULIDADE', 'PROCESSUAL', '-', 'INTIMA\u00c7\u00c3O', 'DO', 'MINIST\u00c9RIO', 'P\u00daBLICO', '-', 'INCAPAZ', 'ACOMPANHADA', 'DE', 'REPRESENTANTE', 'LEGAL', 'E', 'DE', 'ADVOGADO', '-', 'EXERC\u00cdCIO', 'DO', 'CONTRADIT\u00d3RIO', 'E', 'DA', 'AMPLA', 'DEFESA', '-', 'AUS\u00caNCIA', 'DE', 'PREJU\u00cdZOS', '-', 'V\u00cdCIO', 'AFASTADO', '-', 'IMPROCED\u00caNCIA', 'DO', 'PEDIDO', '-', 'INEXIST\u00caNCIA', 'DE', 'PROVA', 'QUANTO', 'AO', 'FATO', 'CONSTITUTIVO', 'DO', 'DIREITO', '.']\n"
          ]
        }
      ],
      "source": [
        "example = datasets[\"train\"][5]\n",
        "print(example[\"tokens\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-vnpI5OEj0M",
        "outputId": "22e1263a-7bb9-4fac-c04e-0d2458642e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'V', '.', 'v', 'AP', '##EL', '##A', '##\u00c7', '##\u00c3O', 'C', '##\u00cd', '##V', '##EL', '-', 'N', '##UL', '##ID', '##AD', '##E', 'PR', '##OC', '##ES', '##S', '##UA', '##L', '-', 'IN', '##TI', '##MA', '##\u00c7', '##\u00c3O', 'DO', 'M', '##IN', '##IS', '##T', '##\u00c9', '##RI', '##O', 'P', '##\u00da', '##B', '##L', '##IC', '##O', '-', 'IN', '##CA', '##PA', '##Z', 'AC', '##OM', '##PA', '##N', '##HA', '##DA', 'DE', 'R', '##EP', '##RE', '##SE', '##NT', '##AN', '##TE', 'L', '##E', '##GA', '##L', 'E', 'DE', 'A', '##D', '##V', '##O', '##GA', '##DO', '-', 'E', '##X', '##ER', '##C', '##\u00cd', '##CI', '##O', 'DO', 'CON', '##T', '##RA', '##DI', '##T', '##\u00d3', '##RI', '##O', 'E', 'D', '##A', 'AM', '##P', '##LA', 'DE', '##F', '##ES', '##A', '-', 'A', '##US', '##\u00ca', '##N', '##CI', '##A', 'DE', 'PR', '##E', '##J', '##U', '##\u00cd', '##Z', '##OS', '-', 'V', '##\u00cd', '##CI', '##O', 'A', '##FA', '##ST', '##AD', '##O', '-', 'I', '##MP', '##RO', '##CE', '##D', '##\u00ca', '##N', '##CI', '##A', 'DO', 'P', '##ED', '##ID', '##O', '-', 'IN', '##E', '##X', '##IS', '##T', '##\u00ca', '##N', '##CI', '##A', 'DE', 'PR', '##O', '##VA', 'Q', '##UA', '##NT', '##O', 'A', '##O', 'FA', '##TO', 'CON', '##ST', '##IT', '##UT', '##IV', '##O', 'DO', 'D', '##IR', '##EI', '##TO', '.', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR_QITMwEj0M"
      },
      "source": [
        "Aqui as palavras \"Zwingmann\" e \"carne de ovelha\" foram divididas em tr\u00eas subtokens.\n",
        "\n",
        "Isso significa que precisamos fazer algum processamento em nossos r\u00f3tulos, pois os ids de entrada retornados pelo tokenizer s\u00e3o maiores que as listas de r\u00f3tulos que nosso conjunto de dados cont\u00e9m, primeiro porque alguns tokens especiais podem ser adicionados (podemos usar um `[CLS]` e um `[SEP]` acima) e por causa dessas poss\u00edveis divis\u00f5es de palavras em v\u00e1rios tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYbaX2MFEj0M",
        "outputId": "0b7954f6-9b63-4298-f2a6-f4e33d4c7d86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 178)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPn53EzhEj0M"
      },
      "source": [
        "Felizmente, o tokenizer retorna sa\u00eddas que possuem um m\u00e9todo `word_ids` que pode nos ajudar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj6SWXNnEj0N",
        "outputId": "3cf69e22-0c9d-40fe-b8c6-90bfc14be857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7, 7, 8, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 14, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 17, 18, 19, 19, 19, 19, 19, 19, 20, 21, 21, 21, 21, 21, 21, 21, 22, 23, 23, 23, 23, 23, 23, 23, 23, 24, 25, 25, 26, 26, 26, 27, 27, 27, 27, 28, 29, 29, 29, 29, 29, 29, 30, 31, 31, 31, 31, 31, 31, 31, 32, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 38, 38, 38, 38, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 42, 42, 42, 43, 43, 43, 43, 44, 44, 45, 45, 46, 46, 46, 46, 46, 46, 47, 48, 48, 48, 48, 49, None]\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_input.word_ids())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0KrSGE4Ej0N"
      },
      "source": [
        "Como podemos ver, ele retorna uma lista com o mesmo n\u00famero de elementos que nossos ids de entrada processados, mapeando tokens especiais para `None` e todos os outros tokens para suas respectivas palavras. Dessa forma, podemos alinhar os r\u00f3tulos com os ids de entrada processados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOEfKf3YEj0O",
        "outputId": "f27a9587-a274-4d88-9bed-f3b88f4082fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "178 178\n"
          ]
        }
      ],
      "source": [
        "word_ids = tokenized_input.word_ids()\n",
        "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
        "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdmkJ1O2Ej0O"
      },
      "source": [
        "Aqui definimos os r\u00f3tulos de todos os tokens especiais como -100 (o \u00edndice que \u00e9 ignorado pelo PyTorch) e os r\u00f3tulos de todos os outros tokens como o r\u00f3tulo da palavra de onde eles v\u00eam. Outra estrat\u00e9gia \u00e9 definir o r\u00f3tulo apenas no primeiro token obtido de uma determinada palavra e atribuir um r\u00f3tulo de -100 aos demais subtokens da mesma palavra. Propomos aqui as duas estrat\u00e9gias, basta alterar o valor da seguinte flag:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG-G4otFEj0P"
      },
      "outputs": [],
      "source": [
        "label_all_tokens = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "Agora estamos prontos para escrever a fun\u00e7\u00e3o que ir\u00e1 pr\u00e9-processar nossas amostras. N\u00f3s os alimentamos no `tokenizer` com o argumento `truncation=True` (para truncar textos maiores que o tamanho m\u00e1ximo permitido pelo modelo) e `is_split_into_words=True` (como visto acima). Em seguida, alinhamos os r\u00f3tulos com os IDs dos tokens usando a estrat\u00e9gia que escolhemos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, max_length=512)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "Esta fun\u00e7\u00e3o funciona com um ou v\u00e1rios exemplos. No caso de v\u00e1rios exemplos, o tokenizer retornar\u00e1 uma lista de listas para cada chave:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b70jh26IrJS",
        "outputId": "c82108f8-7050-4dcf-9878-df6d8f7114de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 192, 7463, 8427, 22301, 131, 12127, 9008, 22301, 22402, 16484, 187, 22360, 22339, 9008, 118, 177, 22402, 16484, 10836, 13760, 7545, 22320, 22323, 22351, 22301, 22402, 16484, 212, 8718, 250, 7665, 6072, 213, 8718, 22301, 6538, 118, 11635, 9008, 13270, 7073, 6765, 118, 11741, 22328, 22341, 6392, 22301, 212, 9008, 22317, 213, 7073, 6538, 22321, 22352, 21748, 22317, 212, 22371, 22318, 22327, 6162, 22317, 192, 22311, 278, 5650, 22341, 257, 5476, 15289, 5903, 22327, 118, 248, 18199, 6392, 11836, 22309, 118, 177, 10409, 22420, 22320, 14298, 22301, 10836, 13760, 16017, 22322, 22339, 12547, 22402, 16484, 15040, 18868, 22322, 22349, 22341, 9208, 248, 22301, 13760, 11846, 22379, 22320, 14298, 22301, 177, 5226, 22341, 22317, 118, 11635, 3341, 12547, 22402, 22301, 10836, 13760, 4529, 5869, 22351, 118, 11635, 22309, 22333, 22341, 22360, 22351, 22317, 192, 22348, 6538, 16017, 8427, 22309, 118, 11635, 9008, 13270, 7073, 6765, 11247, 7918, 22340, 6392, 22301, 118, 248, 18199, 6392, 11836, 22309, 257, 5476, 12234, 22340, 5476, 6392, 22301, 119, 102], [101, 118, 231, 1328, 119, 7048, 117, 1349, 117, 171, 8250, 22304, 543, 1128, 301, 179, 7740, 22279, 320, 3890, 7638, 558, 3718, 529, 8309, 173, 179, 1307, 6399, 125, 22250, 117, 2586, 243, 146, 1328, 119, 2680, 22315, 171, 653, 8178, 179, 146, 1673, 2810, 1444, 326, 625, 146, 3890, 7638, 346, 344, 14353, 201, 221, 12079, 146, 2160, 173, 179, 1981, 558, 3718, 119, 102], [101, 118, 9633, 214, 118, 176, 125, 2973, 18233, 2446, 4977, 1442, 2530, 240, 2760, 12787, 117, 253, 18604, 123, 7418, 171, 3890, 7638, 229, 5061, 125, 13816, 180, 2601, 10629, 119, 102], [101, 118, 8457, 146, 1666, 523, 908, 3828, 286, 423, 5124, 22286, 3660, 17231, 2687, 20477, 162, 117, 1004, 271, 4271, 180, 12385, 125, 8407, 2483, 399, 171, 4794, 170, 1037, 229, 7036, 125, 9176, 1555, 298, 20165, 442, 6960, 17106, 117, 525, 4122, 118, 176, 146, 5035, 180, 1444, 3093, 1076, 171, 2160, 117, 125, 547, 123, 6675, 320, 3890, 7638, 558, 3718, 11925, 202, 2160, 173, 652, 5032, 119, 102], [101, 118, 4407, 4453, 2623, 19903, 328, 221, 11413, 123, 1444, 3093, 1076, 180, 12385, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 9, 10, 10, 10, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "tokenize_and_align_labels(datasets['train'][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "Para aplicar esta fun\u00e7\u00e3o em todas as senten\u00e7as (ou pares de senten\u00e7as) em nosso conjunto de dados, apenas usamos o m\u00e9todo `map` do nosso objeto `dataset` que criamos anteriormente. Isso aplicar\u00e1 a fun\u00e7\u00e3o em todos os elementos de todas as divis\u00f5es no `dataset`, de modo que nossos dados de treinamento, valida\u00e7\u00e3o e teste ser\u00e3o pr\u00e9-processados \u200b\u200bem um \u00fanico comando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "b96e82be9bed460684aba500bd815647",
            "1cdbbbd666f14f9cb7f7fb024177c150",
            "af189972d8e34c36aa8f5da5bba92713",
            "e7fae6492ccf45cb9235eb828e8c3a8a",
            "dc7b789bf47a48eaa7ac7c04d474fdf9",
            "6967d27ac9494642925aec415bf0c6bb",
            "a700ad20370d4c778d3459fe36f9134f",
            "4b0a57b744224848a691324c7b147871",
            "07d45edfdc094a08a505ea721ed2b20f",
            "b0aa6922df2c40a1a41f141e62cc4f58",
            "7ee83e23764d496bae235df6b618c014",
            "3e4169a665d74fa4b00ddee42e3ce2b0",
            "ac3077a8f78e45c89e9b21b3916cefc3",
            "ca1ca85a5a1e46be804d910d4b0dfc07",
            "233bc0ff592c4e0aaa492b2ddaf579ba",
            "04dd72ac55d542dbb1bed3d809e9c45a",
            "a2cbf67ee69b402faf76435223ee01bf",
            "fbbc240b669c4c38abfe5799d55c9611",
            "f6de775a5f5e4ce6b395c8bb387782e7",
            "f611305c6bd1424f90dcf42f50848b17",
            "01d71eea8c3e48caac7ab117e97a145c",
            "b911317e98e64001bf93c4a32fab4157",
            "29fef599e601474bab09027aee36a808",
            "00edef555b7c4641b60a050d07a6a90f",
            "40d3172f5648468eb7210a7cdc3bc2c7",
            "be67628730e144a2addf6da92c4f4262",
            "68b1a71aea8e46b383bea19d995e6312",
            "e0b12a209bf64ed9a33948cfa53692ac",
            "86df70274fed4529bd1526042334cb4a",
            "f22300824ac045c4911a71234bedb391",
            "65d8f59a952f4a43a974590cae3b0d2a",
            "c179d2532f724f58bc2e344e99488457",
            "5eee46330ee34e4389b66d7c5223792d"
          ],
          "height": 113
        },
        "id": "DDtsaJeVIrJT",
        "outputId": "922e0346-62d5-4e78-e5fb-25434851ec9f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7828 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b96e82be9bed460684aba500bd815647"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1177 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e4169a665d74fa4b00ddee42e3ce2b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1390 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29fef599e601474bab09027aee36a808"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "Melhor ainda, os resultados s\u00e3o automaticamente armazenados em cache pela biblioteca \ud83e\udd17 Datasets para evitar perder tempo nesta etapa na pr\u00f3xima vez que voc\u00ea executar seu notebook. A biblioteca \ud83e\udd17 Datasets normalmente \u00e9 inteligente o suficiente para detectar quando a fun\u00e7\u00e3o que voc\u00ea passa para o mapa foi alterada (e, portanto, requer o n\u00e3o uso dos dados do cache). Por exemplo, ele detectar\u00e1 corretamente se voc\u00ea alterar a tarefa na primeira c\u00e9lula e executar novamente o notebook. \ud83e\udd17 Datasets avisa quando usa arquivos em cache, voc\u00ea pode passar `load_from_cache_file=False` na chamada para `map` para n\u00e3o usar os arquivos em cache e for\u00e7ar o pr\u00e9-processamento a ser aplicado novamente.\n",
        "\n",
        "Observe que passamos `batched=True` para codificar os textos em lotes juntos. Isso aproveita todos os benef\u00edcios do tokenizer r\u00e1pido que carregamos anteriormente, que usar\u00e1 multithreading para tratar os textos em um lote simultaneamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Ajustando o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Agora que nossos dados est\u00e3o prontos, podemos baixar o modelo pr\u00e9-treinado e ajust\u00e1-lo. Como todas as nossas tarefas s\u00e3o sobre classifica\u00e7\u00e3o de tokens, usamos a classe `AutoModelForTokenClassification`. Assim como acontece com o tokenizer, o m\u00e9todo `from_pretrained` far\u00e1 o download e armazenar\u00e1 em cache o modelo para n\u00f3s. A \u00fanica coisa que precisamos especificar \u00e9 o n\u00famero de r\u00f3tulos para o nosso problema (que podemos obter dos recursos, como visto antes):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "77df31d9ecd340d381ea616f31a1a5d9",
            "ff8288fab3704edfa567e70896f29bb5",
            "a19a8eb41b4b491db55095739d5ce26c",
            "f1e601bcc4a848efab806d1b541a3ff6",
            "4d8812459c7c4a8aa3cf0e244553c466",
            "6bcc3b42f0a6453e8b42d4fdddbafa1e",
            "a641f6acd6df4bb082b3d0f022a77723",
            "66c991d4a18c4d66bf540f2de718a81f",
            "8d974cf69ff94a499e0d2f8d6d6f1674",
            "f4c7f07478b84300b9aa7f487d074c1f",
            "e8113c6b519341ae806680287686eda6",
            "c9a0a73dabf34ebbbb3f241194a63dda",
            "5bc5f93238044da39c73f50f2353dd9e",
            "8897122d887244cb8f8f3f380f4e9c0b",
            "763ecdd76e5c4669822e2f7db26f2b0b",
            "e47546761ee542efbd7fe593beee259a",
            "15d555a90b734af0a6baf80f12db8145",
            "a0c9fd52aaa74dd0a89041e2b1c896ee",
            "f18ad35b053843f6b3fefc3bcc8352f7",
            "f2053785273c414e8493d79e8221f9c0",
            "4a9e64b6efbb4866962d6c6174cd94e2",
            "ff45434b9a4f415a9087161baa97ab40"
          ]
        },
        "id": "TlqNaB8jIrJW",
        "outputId": "5736a81f-791b-4e90-bd9c-64b00e4f9970"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (\u2026)lve/main/config.json:   0%|          | 0.00/893 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77df31d9ecd340d381ea616f31a1a5d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9a0a73dabf34ebbbb3f241194a63dda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at pierreguillou/bert-base-cased-pt-lenerbr and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "O aviso est\u00e1 nos dizendo que estamos descartando alguns pesos (as camadas `vocab_transform` e `vocab_layer_norm`) e inicializando aleatoriamente alguns outros (as camadas `pre_classifier` e `classifier`). Isso \u00e9 absolutamente normal neste caso, porque estamos removendo o cabe\u00e7ote usado para pr\u00e9-treinar o modelo em um objetivo de modelagem de linguagem mascarada e substituindo-o por um novo cabe\u00e7ote para o qual n\u00e3o temos pesos pr\u00e9-treinados, ent\u00e3o a biblioteca nos avisa que devemos estar bem -ajuste esse modelo antes de us\u00e1-lo para infer\u00eancia, que \u00e9 exatamente o que faremos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "Para instanciar um `Trainer`, precisaremos definir mais tr\u00eas coisas. O mais importante \u00e9 o [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), que \u00e9 uma classe que cont\u00e9m todos os atributos para customizar o treinamento. Requer um nome de pasta, que ser\u00e1 usado para salvar os pontos de verifica\u00e7\u00e3o do modelo, e todos os outros argumentos s\u00e3o opcionais:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjotWD2V9ZOk"
      },
      "outputs": [],
      "source": [
        "#pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGzSZtJD9mf5"
      },
      "outputs": [],
      "source": [
        "#!pip install accelerate==0.20.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "# hyperparameters, which are passed into the training job\n",
        "\n",
        "per_device_batch_size = 4\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "#LR, wd, epochs\n",
        "learning_rate = 1e-4 #2e-5 # (AdamW) we started with 3e-4, then 1e-4, then 5e-5 but the model overfits fastly\n",
        "num_train_epochs = 10 # we started with 10 epochs but the model overfits fastly\n",
        "weight_decay = 0.01\n",
        "fp16 = True\n",
        "\n",
        "# logs\n",
        "logging_steps = 290 # melhor evaluate frequently (5000 seems too high)\n",
        "logging_strategy = 'steps'\n",
        "eval_steps = logging_steps\n",
        "\n",
        "# checkpoints\n",
        "evaluation_strategy = 'epoch' #steps\n",
        "save_total_limit = 1 #3\n",
        "save_strategy = 'epoch' #steps\n",
        "save_steps = 978  #290\n",
        "\n",
        "# best Model\n",
        "load_best_model_at_end = True\n",
        "\n",
        "# folders\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "folder_model = 'e' + str(num_train_epochs) + '_lr' + str(learning_rate)\n",
        "\n",
        "#comentado por conta do armazenamento do google drive\n",
        "output_dir = '/content/drive/MyDrive/' + 'ner-lenerbr-' + str(model_name) + '/checkpoints/' + folder_model\n",
        "logging_dir = '/content/drive/MyDrive/' + 'ner-lenerbr-' + str(model_name) + '/logs/' + folder_model\n",
        "\n",
        "\n",
        "# get best model through a metric\n",
        "metric_for_best_model = 'eval_f1'\n",
        "if metric_for_best_model == 'eval_f1':\n",
        "    greater_is_better = True\n",
        "elif metric_for_best_model == 'eval_loss':\n",
        "    greater_is_better = False\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=per_device_batch_size,\n",
        "    per_device_eval_batch_size=per_device_batch_size*2,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    weight_decay=weight_decay,\n",
        "    save_total_limit=save_total_limit,\n",
        "    logging_steps = logging_steps,\n",
        "    eval_steps = logging_steps,\n",
        "    load_best_model_at_end = load_best_model_at_end,\n",
        "    metric_for_best_model = metric_for_best_model,\n",
        "    greater_is_better = greater_is_better,\n",
        "    gradient_checkpointing = False,\n",
        "    do_train = True,\n",
        "    do_eval = True,\n",
        "    do_predict = True,\n",
        "    evaluation_strategy = evaluation_strategy,\n",
        "    logging_strategy = logging_strategy,\n",
        "    save_strategy = save_strategy,\n",
        "    logging_dir=logging_dir,\n",
        "    save_steps = save_steps,\n",
        "    fp16 = fp16,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR0vqcheeHZV"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers==4.28.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Aqui definimos a avalia\u00e7\u00e3o a ser feita ao final de cada \u00e9poca, ajustamos a taxa de aprendizado, usamos o `batch_size` definido no topo do notebook e customizamos o n\u00famero de \u00e9pocas para treinamento, bem como a redu\u00e7\u00e3o de peso.\n",
        "\n",
        "O \u00faltimo argumento para configurar tudo para que possamos enviar o modelo para o [Hub](https://huggingface.co/models) regularmente durante o treinamento. Remova-o caso n\u00e3o tenha seguido os passos de instala\u00e7\u00e3o na parte superior do notebook. Se voc\u00ea quiser salvar seu modelo localmente em um nome diferente do nome do reposit\u00f3rio que ser\u00e1 enviado, ou se quiser enviar seu modelo para uma organiza\u00e7\u00e3o e n\u00e3o para seu espa\u00e7o de nome, use o argumento `hub_model_id` para definir o nome do reposit\u00f3rio (precisa ser o nome completo, incluindo seu namespace: por exemplo `\"sgugger/bert-finetuned-ner\"` ou `\"huggingface/bert-finetuned-ner\"`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI7gfMErEj0S"
      },
      "source": [
        "Em seguida, precisaremos de um agrupamento de dados que agrupar\u00e1 nossos exemplos processados \u200b\u200benquanto aplica o preenchimento para torn\u00e1-los todos do mesmo tamanho (cada bloco ser\u00e1 preenchido at\u00e9 o comprimento de seu exemplo mais longo). Existe um compilador de dados para esta tarefa na biblioteca Transformers, que n\u00e3o apenas preenche as entradas, mas tamb\u00e9m os r\u00f3tulos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzs981QNEj0S"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UTkNnenEj0S"
      },
      "source": [
        "A \u00faltima coisa a definir para o nosso `Trainer` \u00e9 como calcular as m\u00e9tricas das previs\u00f5es. Aqui carregaremos a m\u00e9trica [`seqeval`](https://github.com/chakki-works/seqeval) (que \u00e9 comumente usada para avaliar resultados no conjunto de dados CONLL) por meio da biblioteca Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "9f89242e28f14063b61c26d4d08a2477",
            "92c76623ebc44e438276761d11714218",
            "72ca3637d7564a62ae354869ce1208eb",
            "2ea301ce61124406a69e1e40066ec0d4",
            "b74fa827b0244978aa0244026595e75f",
            "f3c4a608a3c84c17a416d560f8da2b07",
            "248d4967e19c4d24bb3b7f17866cb232",
            "bfda451575e7403a8ffc05b135ae993a",
            "2d9bf09992994b88b574bfa8315914b7",
            "dec33fe44e6e46f19a0467064a011cc5",
            "f73922ea96ba4a61906d3858c4123eb3"
          ]
        },
        "id": "d0nDnAtiEj0S",
        "outputId": "68d63d5d-681b-4376-f3d1-81f3399744f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-e20ba34f8cc7>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library \ud83e\udd17 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"seqeval\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f89242e28f14063b61c26d4d08a2477"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "metric = load_metric(\"seqeval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKVVExQXEj0S"
      },
      "source": [
        "Esta m\u00e9trica leva uma lista de r\u00f3tulos para as previs\u00f5es e refer\u00eancias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ygxIvr5Ej0T",
        "outputId": "c878cdb9-821d-41eb-f8a5-43c400cc0045"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ORGANIZACAO': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
              " 'overall_precision': 1.0,\n",
              " 'overall_recall': 1.0,\n",
              " 'overall_f1': 1.0,\n",
              " 'overall_accuracy': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
        "metric.compute(predictions=[labels], references=[labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZOdRlRIrJd"
      },
      "source": [
        "Portanto, precisaremos fazer um pouco de p\u00f3s-processamento em nossas previs\u00f5es:\n",
        "- selecione o \u00edndice previsto (com o logit m\u00e1ximo) para cada token\n",
        "- converta-o para seu r\u00f3tulo de string\n",
        "- ignore em todos os lugares onde definimos um r\u00f3tulo de -100\n",
        "\n",
        "A fun\u00e7\u00e3o a seguir faz todo esse p\u00f3s-processamento no resultado de `Trainer.evaluate` (que \u00e9 uma tupla nomeada contendo previs\u00f5es e r\u00f3tulos) antes de aplicar a m\u00e9trica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Observe que descartamos a precis\u00e3o/recupera\u00e7\u00e3o/f1 calculada para cada categoria e nos concentramos apenas na precis\u00e3o/recupera\u00e7\u00e3o/f1/precis\u00e3o geral.\n",
        "\n",
        "Ent\u00e3o s\u00f3 precisamos passar tudo isso junto com nossos conjuntos de dados para o `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "\n",
        "# espere early_stopping_patience x eval_steps antes de interromper o treinamento para obter um modelo melhor\n",
        "early_stopping_patience = 5 #save_total_limit\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "Agora podemos ajustar nosso modelo apenas chamando o m\u00e9todo `train`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "FemIrmhHR5MV",
        "outputId": "f13aab29-85a6-401e-a812-887912b559f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9780' max='9780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9780/9780 27:55, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.089000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.759842</td>\n",
              "      <td>0.826022</td>\n",
              "      <td>0.791551</td>\n",
              "      <td>0.954942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.051600</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.838631</td>\n",
              "      <td>0.811398</td>\n",
              "      <td>0.824790</td>\n",
              "      <td>0.969433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.038300</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.854522</td>\n",
              "      <td>0.841290</td>\n",
              "      <td>0.847854</td>\n",
              "      <td>0.967986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.031700</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.820535</td>\n",
              "      <td>0.818065</td>\n",
              "      <td>0.819298</td>\n",
              "      <td>0.962896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.021900</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.860711</td>\n",
              "      <td>0.874409</td>\n",
              "      <td>0.867506</td>\n",
              "      <td>0.968047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.859623</td>\n",
              "      <td>0.873118</td>\n",
              "      <td>0.866318</td>\n",
              "      <td>0.965974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.011500</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.828669</td>\n",
              "      <td>0.885161</td>\n",
              "      <td>0.855984</td>\n",
              "      <td>0.966584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.006100</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.886562</td>\n",
              "      <td>0.873978</td>\n",
              "      <td>0.880225</td>\n",
              "      <td>0.970911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.896081</td>\n",
              "      <td>0.890108</td>\n",
              "      <td>0.893084</td>\n",
              "      <td>0.975026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.896798</td>\n",
              "      <td>0.891398</td>\n",
              "      <td>0.894090</td>\n",
              "      <td>0.975178</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9780, training_loss=0.031455428913570865, metrics={'train_runtime': 1678.4302, 'train_samples_per_second': 46.639, 'train_steps_per_second': 5.827, 'total_flos': 3811063508492232.0, 'train_loss': 0.031455428913570865, 'epoch': 9.99})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKASz-2vIrJi"
      },
      "source": [
        "O m\u00e9todo `evaluate` permite avaliar novamente no conjunto de dados de avalia\u00e7\u00e3o ou em outro conjunto de dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HljwI_DBSNBn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "92741919-017c-49bb-f67d-c3a8b55fc0db"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='148' max='148' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [148/148 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': nan,\n",
              " 'eval_precision': 0.8967979229770662,\n",
              " 'eval_recall': 0.8913978494623656,\n",
              " 'eval_f1': 0.8940897325280415,\n",
              " 'eval_accuracy': 0.9751778993402106,\n",
              " 'eval_runtime': 8.3243,\n",
              " 'eval_samples_per_second': 141.393,\n",
              " 'eval_steps_per_second': 17.779,\n",
              " 'epoch': 9.99}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwGBhU-lEj0U"
      },
      "source": [
        "Para obter a precis\u00e3o/recall/f1 calculada para cada categoria agora que terminamos o treinamento, podemos aplicar a mesma fun\u00e7\u00e3o de antes no resultado do m\u00e9todo `predict`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhGFZQShSTlu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "d2471060-395a-43af-cd71-519874169127"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'JURISPRUDENCIA': {'precision': 0.8235294117647058,\n",
              "  'recall': 0.745814307458143,\n",
              "  'f1': 0.7827476038338658,\n",
              "  'number': 657},\n",
              " 'LEGISLACAO': {'precision': 0.8959854014598541,\n",
              "  'recall': 0.8598949211908932,\n",
              "  'f1': 0.8775692582663093,\n",
              "  'number': 571},\n",
              " 'LOCAL': {'precision': 0.7148760330578512,\n",
              "  'recall': 0.8917525773195877,\n",
              "  'f1': 0.793577981651376,\n",
              "  'number': 194},\n",
              " 'ORGANIZACAO': {'precision': 0.9028974158183242,\n",
              "  'recall': 0.8604477611940299,\n",
              "  'f1': 0.8811616354604509,\n",
              "  'number': 1340},\n",
              " 'PESSOA': {'precision': 0.9172597864768683,\n",
              "  'recall': 0.9617537313432836,\n",
              "  'f1': 0.9389799635701275,\n",
              "  'number': 1072},\n",
              " 'TEMPO': {'precision': 0.965311004784689,\n",
              "  'recall': 0.9889705882352942,\n",
              "  'f1': 0.9769975786924939,\n",
              "  'number': 816},\n",
              " 'overall_precision': 0.8967979229770662,\n",
              " 'overall_recall': 0.8913978494623656,\n",
              " 'overall_f1': 0.8940897325280415,\n",
              " 'overall_accuracy': 0.9751778993402106}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "I0IKvljqN-9G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "1aa5946b-bb3a-454d-965f-64d57c6c9ba6",
        "id": "VLV4Ju8uN9hW"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'JURISPRUDENCIA': {'precision': 0.8187976291278577,\n",
              "  'recall': 0.8986988847583643,\n",
              "  'f1': 0.8568896765618077,\n",
              "  'number': 1076},\n",
              " 'LEGISLACAO': {'precision': 0.9484346224677717,\n",
              "  'recall': 0.9229390681003584,\n",
              "  'f1': 0.935513169845595,\n",
              "  'number': 558},\n",
              " 'LOCAL': {'precision': 0.5934065934065934,\n",
              "  'recall': 0.7941176470588235,\n",
              "  'f1': 0.6792452830188679,\n",
              "  'number': 68},\n",
              " 'ORGANIZACAO': {'precision': 0.8600823045267489,\n",
              "  'recall': 0.875392670157068,\n",
              "  'f1': 0.8676699532952776,\n",
              "  'number': 955},\n",
              " 'PESSOA': {'precision': 0.9028960817717206,\n",
              "  'recall': 0.9742647058823529,\n",
              "  'f1': 0.9372236958443856,\n",
              "  'number': 544},\n",
              " 'TEMPO': {'precision': 0.9936908517350158,\n",
              "  'recall': 0.9224011713030746,\n",
              "  'f1': 0.9567198177676537,\n",
              "  'number': 683},\n",
              " 'overall_precision': 0.8812375249500998,\n",
              " 'overall_recall': 0.9093717816683831,\n",
              " 'overall_f1': 0.8950836289913836,\n",
              " 'overall_accuracy': 0.9820312614877881}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "results_test = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA3B-BZW8b39"
      },
      "source": [
        "# salvando modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm0QAfx7Q-GT"
      },
      "outputs": [],
      "source": [
        "model_dir = '/content/drive/MyDrive/' + 'ner-lenerbr-' + str(model_name) + '/model/'\n",
        "trainer.save_model(model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbFmH8RvHdro"
      },
      "source": [
        "# FIM"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AreppBrJOK-v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}